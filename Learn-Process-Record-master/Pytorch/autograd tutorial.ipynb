{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random tensors to hold input and outputs\n",
    "# setting requires_grad = False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learninig_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors\n",
    "    # these are exactly the same operations we used to compute the forward pass\n",
    "    # using Tensors, but we do not need to keep references to intermediate\n",
    "    # values since we are not implementing the backward pass by hand\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute and print loss using operations on Tensors\n",
    "    # now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # use autograd to compute the backward pass. This call will compute the \n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient \n",
    "    # of the loss with respect to w1 and w2 respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad = True, but we don't need to track\n",
    "    # this in autograd\n",
    "    \n",
    "    # An alternative way is to operate on weight.data and weight.grad.data\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with \n",
    "    # tensor, but doesn't track history\n",
    "    # Can also use torch.optim.SGD to achieve this\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each primitive autograd operator is really two functions that operate on Tensors.<br>The **forward** function computes output Tensors from input Tensors.<br>The **backward** function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    # implement out own custom autograd Functions by subclassing \n",
    "    # torch.autograd.Function and implementing the forward and backward\n",
    "    # passes which operate on Tensors\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # in the forward pass, we receive a Tensor containing the input and \n",
    "        # return a Tensor containing the output. ctx is a context object that\n",
    "        # can be used to stash information for backward computation.\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # in the backward pass, we receive a Tensor containing the gradient\n",
    "        # of the loss with respect to the output, and we need to compute the \n",
    "        # gradient of the losswith respect to the input\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27505700.0\n",
      "1 21825602.0\n",
      "2 21244482.0\n",
      "3 22264314.0\n",
      "4 22612990.0\n",
      "5 20552628.0\n",
      "6 16185956.0\n",
      "7 11005992.0\n",
      "8 6748862.5\n",
      "9 3937228.0\n",
      "10 2327863.0\n",
      "11 1451868.0\n",
      "12 977546.125\n",
      "13 710063.0\n",
      "14 549152.375\n",
      "15 444335.625\n",
      "16 370681.375\n",
      "17 315479.3125\n",
      "18 272100.34375\n",
      "19 236833.9375\n",
      "20 207559.484375\n",
      "21 182873.28125\n",
      "22 161863.28125\n",
      "23 143809.671875\n",
      "24 128206.203125\n",
      "25 114647.734375\n",
      "26 102814.0234375\n",
      "27 92442.8359375\n",
      "28 83321.6640625\n",
      "29 75284.03125\n",
      "30 68169.890625\n",
      "31 61880.79296875\n",
      "32 56282.70703125\n",
      "33 51284.9140625\n",
      "34 46813.0546875\n",
      "35 42801.6015625\n",
      "36 39195.734375\n",
      "37 35946.73828125\n",
      "38 33012.0625\n",
      "39 30357.421875\n",
      "40 27955.615234375\n",
      "41 25776.55078125\n",
      "42 23795.3515625\n",
      "43 21991.185546875\n",
      "44 20345.64453125\n",
      "45 18842.6953125\n",
      "46 17468.822265625\n",
      "47 16210.923828125\n",
      "48 15057.0166015625\n",
      "49 13997.69921875\n",
      "50 13024.3359375\n",
      "51 12128.6376953125\n",
      "52 11303.6611328125\n",
      "53 10542.896484375\n",
      "54 9840.5078125\n",
      "55 9193.0791015625\n",
      "56 8595.7548828125\n",
      "57 8042.796875\n",
      "58 7530.498046875\n",
      "59 7055.5615234375\n",
      "60 6614.49365234375\n",
      "61 6204.890625\n",
      "62 5824.2099609375\n",
      "63 5470.2216796875\n",
      "64 5140.5341796875\n",
      "65 4833.45458984375\n",
      "66 4547.0400390625\n",
      "67 4279.88916015625\n",
      "68 4031.03466796875\n",
      "69 3798.47998046875\n",
      "70 3581.106201171875\n",
      "71 3377.804443359375\n",
      "72 3187.512939453125\n",
      "73 3009.30517578125\n",
      "74 2842.2099609375\n",
      "75 2685.48388671875\n",
      "76 2538.5\n",
      "77 2400.492919921875\n",
      "78 2270.93798828125\n",
      "79 2149.2041015625\n",
      "80 2034.657470703125\n",
      "81 1927.173828125\n",
      "82 1825.9061279296875\n",
      "83 1730.5626220703125\n",
      "84 1640.7685546875\n",
      "85 1556.193359375\n",
      "86 1476.4512939453125\n",
      "87 1401.197021484375\n",
      "88 1330.2034912109375\n",
      "89 1263.190673828125\n",
      "90 1199.955078125\n",
      "91 1140.160888671875\n",
      "92 1083.6669921875\n",
      "93 1030.240234375\n",
      "94 979.6861572265625\n",
      "95 931.8688354492188\n",
      "96 886.5792236328125\n",
      "97 843.694091796875\n",
      "98 803.0880737304688\n",
      "99 764.62890625\n",
      "100 728.1659545898438\n",
      "101 693.577880859375\n",
      "102 660.7778930664062\n",
      "103 629.65869140625\n",
      "104 600.1447143554688\n",
      "105 572.1149291992188\n",
      "106 545.5028076171875\n",
      "107 520.228759765625\n",
      "108 496.2380065917969\n",
      "109 473.3695373535156\n",
      "110 451.6212158203125\n",
      "111 430.941162109375\n",
      "112 411.280029296875\n",
      "113 392.5858154296875\n",
      "114 374.807373046875\n",
      "115 357.877685546875\n",
      "116 341.75921630859375\n",
      "117 326.41644287109375\n",
      "118 311.81463623046875\n",
      "119 297.90203857421875\n",
      "120 284.64654541015625\n",
      "121 272.0174560546875\n",
      "122 259.9844970703125\n",
      "123 248.51995849609375\n",
      "124 237.58505249023438\n",
      "125 227.17202758789062\n",
      "126 217.23373413085938\n",
      "127 207.74923706054688\n",
      "128 198.70535278320312\n",
      "129 190.072021484375\n",
      "130 181.83450317382812\n",
      "131 173.97091674804688\n",
      "132 166.46450805664062\n",
      "133 159.30319213867188\n",
      "134 152.4615478515625\n",
      "135 145.93072509765625\n",
      "136 139.69046020507812\n",
      "137 133.72760009765625\n",
      "138 128.03494262695312\n",
      "139 122.59406280517578\n",
      "140 117.39378356933594\n",
      "141 112.42292022705078\n",
      "142 107.67172241210938\n",
      "143 103.13375854492188\n",
      "144 98.79216003417969\n",
      "145 94.6472396850586\n",
      "146 90.67816162109375\n",
      "147 86.88067626953125\n",
      "148 83.25062561035156\n",
      "149 79.77680969238281\n",
      "150 76.45256805419922\n",
      "151 73.27369689941406\n",
      "152 70.22975158691406\n",
      "153 67.31881713867188\n",
      "154 64.53372192382812\n",
      "155 61.86606979370117\n",
      "156 59.312339782714844\n",
      "157 56.867950439453125\n",
      "158 54.52839279174805\n",
      "159 52.28904724121094\n",
      "160 50.143978118896484\n",
      "161 48.089942932128906\n",
      "162 46.12176513671875\n",
      "163 44.23692321777344\n",
      "164 42.432044982910156\n",
      "165 40.70401382446289\n",
      "166 39.04856872558594\n",
      "167 37.46258544921875\n",
      "168 35.941802978515625\n",
      "169 34.48461151123047\n",
      "170 33.089111328125\n",
      "171 31.75068473815918\n",
      "172 30.468708038330078\n",
      "173 29.239124298095703\n",
      "174 28.06110382080078\n",
      "175 26.932540893554688\n",
      "176 25.849605560302734\n",
      "177 24.811874389648438\n",
      "178 23.816404342651367\n",
      "179 22.862499237060547\n",
      "180 21.947376251220703\n",
      "181 21.070655822753906\n",
      "182 20.22919464111328\n",
      "183 19.42279815673828\n",
      "184 18.64884376525879\n",
      "185 17.906845092773438\n",
      "186 17.194808959960938\n",
      "187 16.512243270874023\n",
      "188 15.857407569885254\n",
      "189 15.228959083557129\n",
      "190 14.626106262207031\n",
      "191 14.048951148986816\n",
      "192 13.49388313293457\n",
      "193 12.96161937713623\n",
      "194 12.450508117675781\n",
      "195 11.960184097290039\n",
      "196 11.489367485046387\n",
      "197 11.037593841552734\n",
      "198 10.604246139526367\n",
      "199 10.188440322875977\n",
      "200 9.788887977600098\n",
      "201 9.405614852905273\n",
      "202 9.037717819213867\n",
      "203 8.684556007385254\n",
      "204 8.345422744750977\n",
      "205 8.020336151123047\n",
      "206 7.707636833190918\n",
      "207 7.407461166381836\n",
      "208 7.119243144989014\n",
      "209 6.842403411865234\n",
      "210 6.576794624328613\n",
      "211 6.321908473968506\n",
      "212 6.07689094543457\n",
      "213 5.84140157699585\n",
      "214 5.61534309387207\n",
      "215 5.39813756942749\n",
      "216 5.189979076385498\n",
      "217 4.98979377746582\n",
      "218 4.797371864318848\n",
      "219 4.612668514251709\n",
      "220 4.435563087463379\n",
      "221 4.265215873718262\n",
      "222 4.101635456085205\n",
      "223 3.9446287155151367\n",
      "224 3.7938990592956543\n",
      "225 3.648883819580078\n",
      "226 3.5094852447509766\n",
      "227 3.3755781650543213\n",
      "228 3.246955394744873\n",
      "229 3.1234312057495117\n",
      "230 3.004481792449951\n",
      "231 2.8903324604034424\n",
      "232 2.780576705932617\n",
      "233 2.6750621795654297\n",
      "234 2.573620557785034\n",
      "235 2.4761157035827637\n",
      "236 2.382349967956543\n",
      "237 2.2923836708068848\n",
      "238 2.2058005332946777\n",
      "239 2.1226253509521484\n",
      "240 2.0426480770111084\n",
      "241 1.9656541347503662\n",
      "242 1.8917019367218018\n",
      "243 1.8204243183135986\n",
      "244 1.752156138420105\n",
      "245 1.6863731145858765\n",
      "246 1.623063325881958\n",
      "247 1.562264084815979\n",
      "248 1.5038820505142212\n",
      "249 1.4475858211517334\n",
      "250 1.3935432434082031\n",
      "251 1.3414770364761353\n",
      "252 1.2914626598358154\n",
      "253 1.24332594871521\n",
      "254 1.1970651149749756\n",
      "255 1.1525641679763794\n",
      "256 1.1097162961959839\n",
      "257 1.068517804145813\n",
      "258 1.0289725065231323\n",
      "259 0.9908499717712402\n",
      "260 0.9541588425636292\n",
      "261 0.918860137462616\n",
      "262 0.8848707675933838\n",
      "263 0.8522602319717407\n",
      "264 0.8208222389221191\n",
      "265 0.790645956993103\n",
      "266 0.7615163922309875\n",
      "267 0.7335494756698608\n",
      "268 0.7065895795822144\n",
      "269 0.6806373596191406\n",
      "270 0.6556926965713501\n",
      "271 0.6317176818847656\n",
      "272 0.6085484623908997\n",
      "273 0.5863431692123413\n",
      "274 0.5649539232254028\n",
      "275 0.5442339181900024\n",
      "276 0.5244415998458862\n",
      "277 0.5053873062133789\n",
      "278 0.48699626326560974\n",
      "279 0.4692854881286621\n",
      "280 0.4522142708301544\n",
      "281 0.4358622133731842\n",
      "282 0.41998228430747986\n",
      "283 0.4047721028327942\n",
      "284 0.3901251256465912\n",
      "285 0.37605059146881104\n",
      "286 0.3624715805053711\n",
      "287 0.349398136138916\n",
      "288 0.3367747664451599\n",
      "289 0.3246673345565796\n",
      "290 0.31296953558921814\n",
      "291 0.3016965091228485\n",
      "292 0.29088693857192993\n",
      "293 0.28046488761901855\n",
      "294 0.27042049169540405\n",
      "295 0.2606976628303528\n",
      "296 0.2513837218284607\n",
      "297 0.24240824580192566\n",
      "298 0.23376494646072388\n",
      "299 0.22544214129447937\n",
      "300 0.21739652752876282\n",
      "301 0.20967432856559753\n",
      "302 0.2022063136100769\n",
      "303 0.19500699639320374\n",
      "304 0.18811462819576263\n",
      "305 0.1814398467540741\n",
      "306 0.17500899732112885\n",
      "307 0.16881340742111206\n",
      "308 0.16282866895198822\n",
      "309 0.15709583461284637\n",
      "310 0.15154777467250824\n",
      "311 0.14619293808937073\n",
      "312 0.14105719327926636\n",
      "313 0.13608324527740479\n",
      "314 0.13130593299865723\n",
      "315 0.12669944763183594\n",
      "316 0.12224496901035309\n",
      "317 0.11796138435602188\n",
      "318 0.1138230562210083\n",
      "319 0.10985614359378815\n",
      "320 0.10600637644529343\n",
      "321 0.10228532552719116\n",
      "322 0.09873988479375839\n",
      "323 0.0952925980091095\n",
      "324 0.09198813140392303\n",
      "325 0.08880271017551422\n",
      "326 0.08571331202983856\n",
      "327 0.08272695541381836\n",
      "328 0.079866424202919\n",
      "329 0.07710656523704529\n",
      "330 0.07444551587104797\n",
      "331 0.0718780905008316\n",
      "332 0.06939005851745605\n",
      "333 0.066988006234169\n",
      "334 0.064692422747612\n",
      "335 0.062453169375658035\n",
      "336 0.06030906364321709\n",
      "337 0.058239638805389404\n",
      "338 0.05626032501459122\n",
      "339 0.05433427914977074\n",
      "340 0.05248477682471275\n",
      "341 0.05069384351372719\n",
      "342 0.04897186905145645\n",
      "343 0.047304555773735046\n",
      "344 0.04569908231496811\n",
      "345 0.044144727289676666\n",
      "346 0.042632974684238434\n",
      "347 0.041188016533851624\n",
      "348 0.03979715704917908\n",
      "349 0.03844660520553589\n",
      "350 0.037153612822294235\n",
      "351 0.035899631679058075\n",
      "352 0.034690894186496735\n",
      "353 0.03352547809481621\n",
      "354 0.03239424154162407\n",
      "355 0.03131697326898575\n",
      "356 0.03026614338159561\n",
      "357 0.029257358983159065\n",
      "358 0.02827797457575798\n",
      "359 0.027335215359926224\n",
      "360 0.026425551623106003\n",
      "361 0.025534864515066147\n",
      "362 0.024690184742212296\n",
      "363 0.02387162670493126\n",
      "364 0.02308107167482376\n",
      "365 0.022320475429296494\n",
      "366 0.021583985537290573\n",
      "367 0.02087324857711792\n",
      "368 0.020189626142382622\n",
      "369 0.019527774304151535\n",
      "370 0.018884319812059402\n",
      "371 0.01827021688222885\n",
      "372 0.01767396368086338\n",
      "373 0.01710348203778267\n",
      "374 0.01654142141342163\n",
      "375 0.016006451100111008\n",
      "376 0.015484116040170193\n",
      "377 0.014986552298069\n",
      "378 0.014500660821795464\n",
      "379 0.014040987938642502\n",
      "380 0.013583659194409847\n",
      "381 0.01314591709524393\n",
      "382 0.01272527500987053\n",
      "383 0.012314354069530964\n",
      "384 0.011923318728804588\n",
      "385 0.01154683344066143\n",
      "386 0.011175309307873249\n",
      "387 0.010822486132383347\n",
      "388 0.010483256541192532\n",
      "389 0.01015307754278183\n",
      "390 0.009833501651883125\n",
      "391 0.009527054615318775\n",
      "392 0.009222904220223427\n",
      "393 0.008933810517191887\n",
      "394 0.008659644052386284\n",
      "395 0.008384846150875092\n",
      "396 0.00812546443194151\n",
      "397 0.00787281058728695\n",
      "398 0.007626032922416925\n",
      "399 0.007390213198959827\n",
      "400 0.007166164927184582\n",
      "401 0.00694533484056592\n",
      "402 0.006734687834978104\n",
      "403 0.0065300352871418\n",
      "404 0.006332040764391422\n",
      "405 0.006139402277767658\n",
      "406 0.005954289808869362\n",
      "407 0.0057741268537938595\n",
      "408 0.005604811944067478\n",
      "409 0.005436802748590708\n",
      "410 0.0052749989554286\n",
      "411 0.005120683461427689\n",
      "412 0.004968839697539806\n",
      "413 0.004820863250643015\n",
      "414 0.004680621437728405\n",
      "415 0.0045410036109387875\n",
      "416 0.0044069113209843636\n",
      "417 0.004277733154594898\n",
      "418 0.0041555920615792274\n",
      "419 0.004033188801258802\n",
      "420 0.003914705477654934\n",
      "421 0.003800734644755721\n",
      "422 0.0036952567752450705\n",
      "423 0.0035912436433136463\n",
      "424 0.0034869532100856304\n",
      "425 0.003388259094208479\n",
      "426 0.0032924495171755552\n",
      "427 0.003201661631464958\n",
      "428 0.0031147440895438194\n",
      "429 0.0030270537827163935\n",
      "430 0.002942245453596115\n",
      "431 0.002860453212633729\n",
      "432 0.002779969945549965\n",
      "433 0.0027050734497606754\n",
      "434 0.002632815856486559\n",
      "435 0.002559974789619446\n",
      "436 0.0024923477321863174\n",
      "437 0.002422588411718607\n",
      "438 0.0023579190019518137\n",
      "439 0.0022967029362916946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440 0.002232269151136279\n",
      "441 0.002174162771552801\n",
      "442 0.002116383984684944\n",
      "443 0.002060707425698638\n",
      "444 0.0020055449567735195\n",
      "445 0.0019547506235539913\n",
      "446 0.0019014577846974134\n",
      "447 0.0018518504220992327\n",
      "448 0.0018041272414848208\n",
      "449 0.0017574866069480777\n",
      "450 0.0017128109466284513\n",
      "451 0.0016697099199518561\n",
      "452 0.0016252878122031689\n",
      "453 0.0015859502600505948\n",
      "454 0.0015450258506461978\n",
      "455 0.00150502217002213\n",
      "456 0.0014686014037579298\n",
      "457 0.001433422788977623\n",
      "458 0.00139753264375031\n",
      "459 0.0013621656689792871\n",
      "460 0.0013291677460074425\n",
      "461 0.0012957497965544462\n",
      "462 0.0012626450043171644\n",
      "463 0.0012348862364888191\n",
      "464 0.0012039887951686978\n",
      "465 0.001175607554614544\n",
      "466 0.0011470505269244313\n",
      "467 0.0011189209762960672\n",
      "468 0.0010934759629890323\n",
      "469 0.0010664132423698902\n",
      "470 0.0010416917502880096\n",
      "471 0.0010180867975577712\n",
      "472 0.00099430070258677\n",
      "473 0.0009708348079584539\n",
      "474 0.0009476985433138907\n",
      "475 0.0009266933193430305\n",
      "476 0.0009064298938028514\n",
      "477 0.0008854640764184296\n",
      "478 0.0008662462932989001\n",
      "479 0.0008463396225124598\n",
      "480 0.0008276125299744308\n",
      "481 0.0008090243791230023\n",
      "482 0.0007899276679381728\n",
      "483 0.0007731576915830374\n",
      "484 0.0007561398088000715\n",
      "485 0.0007410903344862163\n",
      "486 0.0007236030651256442\n",
      "487 0.0007073646993376315\n",
      "488 0.0006923386827111244\n",
      "489 0.0006777832168154418\n",
      "490 0.0006644331151619554\n",
      "491 0.0006502401083707809\n",
      "492 0.0006359480321407318\n",
      "493 0.0006227779667824507\n",
      "494 0.0006099398015066981\n",
      "495 0.0005974413361400366\n",
      "496 0.0005852701724506915\n",
      "497 0.0005730486591346562\n",
      "498 0.0005611771484836936\n",
      "499 0.0005503187421709299\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# create random Tensors for weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
